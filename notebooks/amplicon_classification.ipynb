{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Classifiers\n",
    "\n",
    "Neural networks are very flexible algorithms which have revolutionized artificial intelligence in the last few years. Their success is due to new tricks in designing and training the networks, but also the availability of large datasets and the computing power to process them.\n",
    "\n",
    "In this notebook we will look at some simple networks and how they deal with the data we prepared earlier. When working with machine learning, it is a good idea to start with simpler models, and only proceed to more sophisticated methods when you know the limitations of the simple models have been reached.\n",
    "\n",
    "I haven't spent much time tweaking the hyperparameters of these models, so there may be a few additional percent of accuracy here and there, which can be found by tweaking the models a little. More so if a more systematic approach to optimize them was to be used.\n",
    "\n",
    "Another problem is that the test data was used to optimize the models and their hyperparameters. This introduces a certain amount of \"data snooping\" and slightly defies the purpose of the out-of-sample performance validation. I decided to make this tradeoff because there is already very few data, and splitting off another test set would have hampered performance further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import pysam\n",
    "import random\n",
    "import feather\n",
    "import h5py\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_data = feather.read_dataframe(\"amplicon_training_metadata.feather\")\n",
    "test_data = feather.read_dataframe(\"amplicon_test_metadata.feather\")\n",
    "h5f = h5py.File(\"amplicon_dataset.hdf\", \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_X = h5f[\"training/X\"][:]\n",
    "training_y = h5f[\"training/y\"][:]\n",
    "\n",
    "test_X = h5f[\"test/X\"][:]\n",
    "test_y = h5f[\"test/y\"][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate_model(model):\n",
    "    y_hat = model.predict(training_X).argmax(axis=1)\n",
    "    training_accuracy = (y_hat == training_y.argmax(axis=1)).sum() / len(training_y)\n",
    "\n",
    "    y_hat = model.predict(test_X).argmax(axis=1)\n",
    "    test_accuracy = (y_hat == test_y.argmax(axis=1)).sum() / len(test_y)\n",
    "    \n",
    "    return  training_accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers.core import Dense, Activation, Dropout,  Flatten\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
    "from keras.regularizers import l2, activity_l2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Relu Layer\n",
    "\n",
    "The first model consists only of one layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(input_dim=200, output_dim=100, init=\"glorot_uniform\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(output_dim=11, init=\"glorot_uniform\"))\n",
    "model.add(Activation(\"softmax\"))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.05, momentum=0.05, nesterov=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.77 0.55\n",
      "0.86 0.61\n",
      "0.80 0.57\n",
      "0.87 0.58\n",
      "0.89 0.54\n",
      "0.84 0.52\n",
      "0.76 0.48\n",
      "0.97 0.55\n",
      "0.98 0.55\n",
      "0.98 0.54\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    model.fit(training_X, training_y, nb_epoch=10, batch_size=64,verbose=False)\n",
    "    print ( \"%.2f %.2f\" % evaluate_model(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first column shows the accuracy on the training set, the second on the test set. As we can see, this model performs stunningly well on the training set, but quite bad on the test set.\n",
    "\n",
    "This is due to overfitting.\n",
    "\n",
    "But: Because there are 11 amplicons, 56% isn't disheartiningly bad either.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Layers, with dropout\n",
    "\n",
    "The second model contains two layers. Dropout is activated for both of them, in order to reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(input_dim=200, output_dim=100, init=\"glorot_uniform\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(input_dim=200, output_dim=100, init=\"glorot_uniform\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(output_dim=11, init=\"glorot_uniform\"))\n",
    "model.add(Activation(\"softmax\"))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.05, momentum=0.05, nesterov=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75 0.56\n",
      "0.80 0.58\n",
      "0.81 0.58\n",
      "0.82 0.59\n",
      "0.83 0.59\n",
      "0.83 0.58\n",
      "0.84 0.59\n",
      "0.83 0.58\n",
      "0.84 0.58\n",
      "0.85 0.60\n",
      "0.85 0.58\n",
      "0.85 0.58\n",
      "0.85 0.58\n",
      "0.85 0.59\n",
      "0.86 0.59\n",
      "0.85 0.58\n",
      "0.86 0.59\n",
      "0.85 0.57\n",
      "0.86 0.59\n",
      "0.85 0.57\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    model.fit(training_X, training_y, nb_epoch=40, batch_size=64,verbose=False)\n",
    "    print (\"%.2f %.2f\" % evaluate_model(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model has a significantly harder time to learn/overfit the training set. In turn the test performance is a bit better, but still not useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation\n",
    "\n",
    "When the training performance is high and the test performance is low, it's often a problem of too little data. But what do we do if we don't have enough data? Well, we can just make it up.\n",
    "\n",
    "The idea is to just add a bit of noise to the event data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(input_dim=200, output_dim=200, init=\"glorot_uniform\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(input_dim=200, output_dim=200, init=\"glorot_uniform\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(output_dim=11, init=\"glorot_uniform\"))\n",
    "model.add(Activation(\"softmax\"))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.05, momentum=0.05, nesterov=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def batch_feeder():\n",
    "    nb = 200\n",
    "    while 1:        \n",
    "        index = np.arange(len(training_X))\n",
    "        np.random.shuffle(index)\n",
    "        for i in np.arange(0,len(index),nb):\n",
    "            sub = index[i:i+nb]\n",
    "            X = training_X[sub,:]\n",
    "            n,m = X.shape\n",
    "            X += np.random.normal(0,0.5,size=(n,m)) * np.random.normal(1,0.1) + np.random.normal(0,0.1)\n",
    "            y = training_y[sub,:]\n",
    "            yield X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95 0.78\n",
      "0.95 0.78\n",
      "0.96 0.78\n",
      "0.96 0.78\n",
      "0.96 0.78\n",
      "0.97 0.79\n",
      "0.97 0.78\n",
      "0.97 0.78\n",
      "0.97 0.79\n",
      "0.97 0.79\n",
      "0.97 0.79\n",
      "0.97 0.79\n",
      "0.97 0.79\n",
      "0.97 0.80\n",
      "0.97 0.79\n",
      "0.97 0.80\n",
      "0.98 0.80\n",
      "0.98 0.79\n",
      "0.98 0.79\n",
      "0.98 0.80\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    model.fit_generator(batch_feeder(), samples_per_epoch=1000000, nb_epoch=1, verbose=0)\n",
    "    print (\"%.2f %.2f\" % evaluate_model(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test performance approaches 80%. Still not good enough, but we are getting there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks\n",
    "\n",
    "One of the most important \"tricks\" that led to the boom of \"deep learning\" is the idea of Convolutional Neural Networks. These networks train small feature detectors, for example with a width of three elements and apply these on the full length of the input multiple times. This achieves translational invariance, which means it can recognize patterns independently of where they are occurring, whereas a normal network can only recognize patterns at exactly the same place where they were learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# implementational detail: 1D Convolutional layers in keras expect inputs to have three dimensions\n",
    "\n",
    "def reshape3(X):\n",
    "    n,m = X.shape\n",
    "    Xn = X.copy()\n",
    "    Xn.shape = (n,m,1)\n",
    "    return Xn\n",
    "\n",
    "def batch_feeder():\n",
    "    nb = 50\n",
    "    while 1:        \n",
    "        index = np.arange(len(training_X))\n",
    "        np.random.shuffle(index)\n",
    "        for i in np.arange(0,len(index),nb):\n",
    "            sub = index[i:i+nb]\n",
    "            X = training_X[sub,:]\n",
    "            n,m = X.shape\n",
    "            X += np.random.normal(0,0.5,size=(n,m)) * np.random.normal(1,0.05) + np.random.normal(0,0.1)\n",
    "            X = reshape3(X)\n",
    "            y = training_y[sub,:]\n",
    "            yield (X, y)\n",
    "\n",
    "training_X3 = reshape3(training_X)[:3000,:]\n",
    "test_X3 = reshape3(test_X)\n",
    "def evaluate_model(model):\n",
    "    y_hat = model.predict(training_X3).argmax(axis=1)\n",
    "    training_accuracy = (y_hat == training_y[:3000,:].argmax(axis=1)).sum() / 3000\n",
    "\n",
    "    y_hat = model.predict(test_X3).argmax(axis=1)\n",
    "    test_accuracy = (y_hat == test_y.argmax(axis=1)).sum() / len(test_y)\n",
    "    \n",
    "    return  training_accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Convolution1D(nb_filter=20,\n",
    "                        filter_length=3,\n",
    "                        border_mode='valid',\n",
    "                        activation='relu',\n",
    "                        subsample_length=1,\n",
    "                        input_shape=(200,1),\n",
    "                        W_regularizer= l2(0.01),\n",
    "                       ))\n",
    "model.add(Convolution1D(nb_filter=20,\n",
    "                        filter_length=3,\n",
    "                        border_mode='valid',\n",
    "                        activation='relu',\n",
    "                        subsample_length=1,\n",
    "                        input_shape=(200,1),\n",
    "                        W_regularizer= l2(0.01),\n",
    "                       ))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(MaxPooling1D(pool_length=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(output_dim=50, init=\"glorot_uniform\"))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dense(output_dim=11, init=\"glorot_uniform\"))\n",
    "model.add(Activation(\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.05, momentum=0.05, nesterov=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.94 0.83\n",
      "0.94 0.82\n",
      "0.93 0.81\n",
      "0.94 0.83\n",
      "0.94 0.82\n",
      "0.94 0.82\n",
      "0.95 0.83\n",
      "0.94 0.82\n",
      "0.94 0.81\n",
      "0.94 0.82\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    model.fit_generator(batch_feeder(), samples_per_epoch=20000, nb_epoch=1, verbose=0)\n",
    "    print (\"%.2f %.2f\" % evaluate_model(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model takes the longest to train. The best performance I have seen was 85%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Neural networks provide a way to classify nanopore reads before basecalling and without any alignment. Such classification with neural networks are extremely fast, making them ideal for Read Until applications, realtime monitoring and embedded devices.\n",
    "\n",
    "However, they require a lot of real data. In this project the best out-of-sample performance reached was 85%. This accuracy may be enough to be useful in some applications, but accuracies above 90% would greatly increase usefulness, of course.\n",
    "\n",
    "Because of other experiments, which I haven't written up yet, I do believe accuracies beyond 90% should be possible in this setting, if more data is available. Specifically, I used other datasets from [Loose et al.] to predict which of five regions of the lambda region a read is from, and achieved an accuracy of up to 95%, from segments of 500 events.\n",
    "\n",
    "\n",
    "The longer the segment which is used for the classification, the more accurate the classifier can be. In this case I used a segment of size 200 to demonstrate a relatively low baseline. In Read Until applications, the longer the segment used for classification is, the lower the \"gain\" from the Read Until strategy will be.\n",
    "\n",
    "In a situation where 10% of the reads are desireable, a Read Until strategy with an accuracy of 80% could increase the desireable output to up to 30%, but only if the reads are long enough.\n",
    "\n",
    "Neural Networks of this sort are also a good choice for embedded devices with nanopore chips as DNA sensors, because they require significantly less memory and computing power than the basecallers (HMM or RNN) will, but they can still answer simple questions like detect a certain microbe, RNA product or the like.\n",
    "\n",
    "Other ideas for classification:\n",
    "* Human vs procaryote\n",
    "* Genomic vs mitochondrial\n",
    "* High quality vs low quality/damaged DNA\n",
    "* Will align/won't align\n",
    "* Barcoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
